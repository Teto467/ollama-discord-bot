# --- START OF FILE bot.py ---

import asyncio
import sys
import os
import json
import logging
import datetime
import time
from collections import defaultdict, deque
import math # NaNãƒã‚§ãƒƒã‚¯ç”¨ã«è¿½åŠ 
import functools # run_in_executorç”¨ã«è¿½åŠ 

import discord
from discord import app_commands, Embed
from discord.ext import commands, tasks # tasksã‚’è¿½åŠ 
from dotenv import load_dotenv
import aiohttp
try:
    import aiofiles # éåŒæœŸãƒ•ã‚¡ã‚¤ãƒ«I/Oç”¨
except ImportError:
    aiofiles = None # ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—æ™‚ã®ãƒ•ãƒ©ã‚°

# Gemini API é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import google.generativeai as genai
    from google.api_core import exceptions as google_exceptions # Gemini APIã‚¨ãƒ©ãƒ¼å‡¦ç†ç”¨
    from google.generativeai.types import HarmCategory, HarmBlockThreshold # ã‚»ãƒ¼ãƒ•ãƒ†ã‚£è¨­å®šç”¨
except ImportError:
    genai = None
    google_exceptions = None
    # loggerã¯ã¾ã å®šç¾©ã•ã‚Œã¦ã„ãªã„ã®ã§ã€ã“ã“ã§ã¯printã™ã‚‹
    # ã“ã®è­¦å‘Šã¯å¾Œã»ã©loggerãŒåˆæœŸåŒ–ã•ã‚ŒãŸå¾Œã«ã‚‚å‡ºã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
    print("è­¦å‘Š: 'google-generativeai' ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Gemini APIæ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")


# --- Windowsç”¨ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãƒãƒªã‚·ãƒ¼ã®è¨­å®š ---
if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# --- ãƒ­ã‚®ãƒ³ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
        # logging.FileHandler("bot.log", encoding="utf-8") # å¿…è¦ã«å¿œã˜ã¦æœ‰åŠ¹åŒ–
    ]
)
logger = logging.getLogger('discord_llm_bot') # BOTåã‚’discord_llm_botã«å¤‰æ›´

# --- ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ ---
load_dotenv()
TOKEN = os.getenv('DISCORD_TOKEN')
OLLAMA_API_URL = os.getenv('OLLAMA_API_URL', 'http://localhost:11434')
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY') # Gemini APIã‚­ãƒ¼
DEFAULT_MODEL = os.getenv('DEFAULT_MODEL') # ä¾‹: "ollama:llama3" ã‚„ "gemini:gemini-1.5-flash-latest"

try:
    CHAT_CHANNEL_ID = int(os.getenv('CHAT_CHANNEL_ID'))
except (TypeError, ValueError):
    logger.error("ç’°å¢ƒå¤‰æ•° 'CHAT_CHANNEL_ID' ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ã‹ã€æ•´æ•°å€¤ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚BOTã‚’çµ‚äº†ã—ã¾ã™ã€‚")
    sys.exit(1)
try:
    HISTORY_LIMIT = int(os.getenv('HISTORY_LIMIT', '50'))
except ValueError:
    logger.warning("ç’°å¢ƒå¤‰æ•° 'HISTORY_LIMIT' ã®å€¤ãŒä¸æ­£ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®50ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    HISTORY_LIMIT = 50
try:
    PROMPT_RELOAD_INTERVAL_MINUTES = float(os.getenv('PROMPT_RELOAD_INTERVAL', '5.0'))
except ValueError:
    logger.warning("ç’°å¢ƒå¤‰æ•° 'PROMPT_RELOAD_INTERVAL' ã®å€¤ãŒä¸æ­£ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®5.0åˆ†ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    PROMPT_RELOAD_INTERVAL_MINUTES = 5.0
try:
    MODEL_UPDATE_INTERVAL_MINUTES = float(os.getenv('MODEL_UPDATE_INTERVAL', '15.0'))
except ValueError:
    logger.warning("ç’°å¢ƒå¤‰æ•° 'MODEL_UPDATE_INTERVAL' ã®å€¤ãŒä¸æ­£ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®15.0åˆ†ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    MODEL_UPDATE_INTERVAL_MINUTES = 15.0

# --- Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š ---
if genai and GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        logger.info("Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šå®Œäº†ã€‚")
    except Exception as e:
        logger.error(f"Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šå¤±æ•—: {e}", exc_info=True)
        genai = None # è¨­å®šå¤±æ•—æ™‚ã¯genaiã‚’Noneã«ã—ã¦æ©Ÿèƒ½ç„¡åŠ¹åŒ–
elif genai and not GEMINI_API_KEY:
    logger.warning("ç’°å¢ƒå¤‰æ•° 'GEMINI_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Gemini APIæ©Ÿèƒ½ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¾ã™ã€‚")
    genai = None
elif not genai: # ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—æ™‚ã®å†è­¦å‘Š (loggerãŒåˆ©ç”¨å¯èƒ½ã«ãªã£ãŸãŸã‚)
    logger.warning("'google-generativeai' ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ãŸãŸã‚ã€Gemini APIæ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")


# --- BOTè¨­å®š ---
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix='!', intents=intents) # ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã¯ç¾çŠ¶ã»ã¼ä½¿ã‚ãªã„ãŒä¸€å¿œæ®‹ã™

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° & å®šæ•° ---
active_model: str | None = DEFAULT_MODEL # ç¾åœ¨é¸æŠä¸­ã®ãƒ¢ãƒ‡ãƒ« (ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ã: "ollama:model" or "gemini:model")

# å„ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿æŒ (None ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨)
# ã‚­ãƒ¼ã¯ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ããƒ¢ãƒ‡ãƒ«å (ä¾‹: "ollama:llama3", "gemini:gemini-1.5-pro-latest")
system_prompts: dict[str, str | None] = defaultdict(lambda: None)

PROMPT_DIR_NAME = "prompts"
available_prompts: dict[str, str] = {} # prompts ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

# ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ (ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ããƒ¢ãƒ‡ãƒ«å: "ollama:model" or "gemini:model")
available_bot_models: list[str] = []

PROMPT_NAME_DEFAULT = "[ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ]"

channel_data = defaultdict(lambda: {
    "history": deque(maxlen=HISTORY_LIMIT),
    "params": {"temperature": 0.7, "top_k": None, "top_p": None}, # top_k, top_p ã‚‚è¿½åŠ  (Noneã¯æœªè¨­å®š)
    "stats": deque(maxlen=50),
    "is_generating": False,
    "stop_generation_requested": False,
})

STREAM_UPDATE_INTERVAL = 1.5 # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ™‚ã®Discordãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ›´æ–°é–“éš”ï¼ˆç§’ï¼‰
STREAM_UPDATE_CHARS = 75    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ™‚ã®Discordãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ›´æ–°æ–‡å­—æ•°é–“éš”

script_dir = os.path.dirname(os.path.abspath(__file__))
prompts_dir_path = os.path.join(script_dir, PROMPT_DIR_NAME)

DEFAULT_SYSTEM_PROMPT_TEXT = "" # APIå´ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ã†æ„å›³

# Geminiç”¨ã®ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã‚»ãƒƒãƒ†ã‚£ãƒ³ã‚° (å¿…è¦ã«å¿œã˜ã¦èª¿æ•´)
GEMINI_SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
}


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def get_model_type_and_name(model_identifier: str | None) -> tuple[str | None, str | None]:
    """
    ãƒ¢ãƒ‡ãƒ«è­˜åˆ¥å­ (ä¾‹: "ollama:llama3", "gemini:gemini-pro") ã‹ã‚‰ã‚¿ã‚¤ãƒ—ã¨å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«åã‚’åˆ†é›¢ã™ã‚‹ã€‚
    ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒãªã„å ´åˆã¯ã€ã‚¿ã‚¤ãƒ—ã‚’Noneã€åå‰ã‚’ãã®ã¾ã¾è¿”ã™ï¼ˆå¾Œæ–¹äº’æ›æ€§ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ï¼‰ã€‚
    """
    if not model_identifier:
        return None, None
    if ":" in model_identifier:
        parts = model_identifier.split(":", 1)
        if len(parts) == 2:
            return parts[0].lower(), parts[1]
    return None, model_identifier # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãªã— (Ollamaå˜ç‹¬æ™‚ä»£ã®åæ®‹ã‚„ã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§)

def get_prompt_name_from_content(prompt_content: str | None) -> str:
    if prompt_content is None or prompt_content == DEFAULT_SYSTEM_PROMPT_TEXT:
        return PROMPT_NAME_DEFAULT
    for name, content in available_prompts.items():
        if prompt_content == content:
            return name
    return "[ã‚«ã‚¹ã‚¿ãƒ è¨­å®š]"

def _load_prompts_sync(dir_path: str) -> dict[str, str]:
    loaded_prompts = {}
    logger.debug(f"_load_prompts_sync: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{dir_path}' ã®åŒæœŸèª­ã¿è¾¼ã¿ã‚’é–‹å§‹...")
    if not os.path.isdir(dir_path):
        logger.warning(f"_load_prompts_sync: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{dir_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return {}
    try:
        for filename in os.listdir(dir_path):
            if filename.lower().endswith(".txt"):
                file_path = os.path.join(dir_path, filename)
                prompt_name = os.path.splitext(filename)[0]
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read().strip()
                        if content:
                            if prompt_name == PROMPT_NAME_DEFAULT: # äºˆç´„èªãƒã‚§ãƒƒã‚¯
                                logger.warning(f"  - _sync: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå '{prompt_name}' ({filename}) ã¯äºˆç´„èªã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
                                continue
                            loaded_prompts[prompt_name] = content
                            logger.debug(f"  - _sync: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ '{prompt_name}' ã‚’èª­ã¿è¾¼ã¿ã€‚")
                        else:
                            logger.warning(f"  - _sync: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã¯ç©ºã€‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
                except Exception as e:
                    logger.error(f"  - _sync: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{filename}' èª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}", exc_info=False)
    except Exception as e:
        logger.error(f"_sync: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{dir_path}' ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
    logger.debug(f"_load_prompts_sync: åŒæœŸèª­ã¿è¾¼ã¿å®Œäº†: {len(loaded_prompts)} å€‹ã€‚")
    return loaded_prompts

async def fetch_and_update_available_models() -> list[str]:
    """Ollamaã¨Gemini APIã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ä¸€è¦§ã‚’å–å¾—ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°ã™ã‚‹"""
    global available_bot_models
    new_model_list = []

    # 1. Ollamaãƒ¢ãƒ‡ãƒ«ã®å–å¾—
    ollama_url = f"{OLLAMA_API_URL}/api/tags"
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(ollama_url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                if response.status == 200:
                    data = await response.json()
                    models = data.get('models', [])
                    for model_info in models:
                        model_name = model_info.get('name')
                        if model_name:
                            new_model_list.append(f"ollama:{model_name}")
                    logger.info(f"Ollamaã‹ã‚‰ {len(models)} å€‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã€‚")
                else:
                    logger.warning(f"Ollamaãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—APIã‚¨ãƒ©ãƒ¼ - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status}, URL: {ollama_url}")
    except asyncio.TimeoutError:
        logger.error(f"Ollama API ({ollama_url}) ã¸ã®æ¥ç¶šãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ (ãƒ¢ãƒ‡ãƒ«å–å¾—æ™‚)ã€‚")
    except aiohttp.ClientConnectorError as e:
        logger.error(f"Ollama APIã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ (ãƒ¢ãƒ‡ãƒ«å–å¾—æ™‚): {e}. URL: {ollama_url}")
    except Exception as e:
        logger.error(f"Ollamaãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã®å–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

    # 2. Geminiãƒ¢ãƒ‡ãƒ«ã®å–å¾—
    if genai: # Gemini APIãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿
        try:
            gemini_models_found = 0
            for model_info in genai.list_models():
                # 'generateContent' (ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ) ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€ã‹ã¤åå‰ã« 'embedding' ã‚’å«ã¾ãªã„ãƒ¢ãƒ‡ãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
                if 'generateContent' in model_info.supported_generation_methods and 'embedding' not in model_info.name:
                    # ãƒ¢ãƒ‡ãƒ«åã¯é€šå¸¸ "models/gemini-1.5-pro-latest" ã®ã‚ˆã†ãªå½¢å¼ãªã®ã§ã€"gemini-" ã‚ˆã‚Šå¾Œã‚’å–å¾—
                    name_part = model_info.name.split('/')[-1]
                    new_model_list.append(f"gemini:{name_part}")
                    gemini_models_found +=1
            logger.info(f"Gemini APIã‹ã‚‰ {gemini_models_found} å€‹ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã€‚")
        except google_exceptions.GoogleAPIError as e:
            logger.error(f"Gemini APIã‹ã‚‰ã®ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ã«å¤±æ•—: {e}")
        except Exception as e:
            logger.error(f"Geminiãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã®å–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
    else:
        logger.info("Gemini APIãŒç„¡åŠ¹ãªãŸã‚ã€Geminiãƒ¢ãƒ‡ãƒ«ã®å–å¾—ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")

    sorted_models = sorted(list(set(new_model_list))) # é‡è¤‡é™¤å»ã¨ã‚½ãƒ¼ãƒˆ

    if sorted_models != available_bot_models:
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆæ›´æ–° ({len(sorted_models)}å€‹): {sorted_models}")
        available_bot_models = sorted_models
    else:
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆå¤‰æ›´ãªã—ã€‚ç¾åœ¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {len(available_bot_models)}å€‹ã€‚")
    return available_bot_models


async def fetch_channel_history(channel: discord.TextChannel, limit: int = 100):
    if not isinstance(channel, discord.TextChannel):
        logger.warning(f"æŒ‡å®šã•ã‚ŒãŸãƒãƒ£ãƒ³ãƒãƒ«ãŒç„¡åŠ¹ã§ã™: {channel}")
        return

    channel_id = channel.id
    logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ« '{channel.name}' (ID: {channel_id}) ã®å±¥æ­´å–å¾—ã‚’é–‹å§‹ (æœ€å¤§{limit}ä»¶)...")
    try:
        messages_to_add = []
        count = 0
        async for message in channel.history(limit=limit):
            if not message.author.bot or message.author.id == bot.user.id: # è‡ªåˆ†è‡ªèº«ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯å«ã‚ã‚‹
                if message.content: # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿
                    messages_to_add.append({
                        "author_name": message.author.display_name,
                        "author_id": message.author.id,
                        "content": message.content,
                        "timestamp": message.created_at.isoformat(),
                        "is_bot": message.author.bot # BOTè‡ªèº«ã®ç™ºè¨€ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°
                    })
                    count += 1
        
        added_count = 0
        history_deque = channel_data[channel_id]["history"]
        # æ—¢å­˜ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è­˜åˆ¥ã™ã‚‹ãŸã‚ã®ã‚»ãƒƒãƒˆ (timestampã¨contentã®ã‚¿ãƒ—ãƒ«)
        existing_timestamps_contents = { (msg["timestamp"], msg["content"]) for msg in history_deque }

        for msg in reversed(messages_to_add): # æ–°ã—ã„ã‚‚ã®ã‹ã‚‰é †ã«è¿½åŠ ã™ã‚‹ãŸã‚ã«é€†é †ã§å‡¦ç†
            if (msg["timestamp"], msg["content"]) not in existing_timestamps_contents:
                 history_deque.append(msg)
                 existing_timestamps_contents.add((msg["timestamp"], msg["content"]))
                 added_count += 1
        
        logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ« '{channel.name}' ã‹ã‚‰ {count} ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’èª¿æŸ»ã—ã€{added_count} ä»¶ã‚’å±¥æ­´ã«è¿½åŠ ã—ã¾ã—ãŸã€‚ç¾åœ¨ã®å±¥æ­´æ•°: {len(history_deque)}")

    except discord.Forbidden:
        logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« '{channel.name}' ã®å±¥æ­´èª­ã¿å–ã‚Šæ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    except discord.HTTPException as e:
        logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« '{channel.name}' ã®å±¥æ­´å–å¾—ä¸­ã«Discord APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    except Exception as e:
        logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« '{channel.name}' ã®å±¥æ­´å–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)


def build_ollama_chat_context(channel_id: int) -> list[dict]:
    """Ollama APIç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹"""
    history_deque = channel_data[channel_id]["history"]
    messages = []
    for msg in history_deque:
        # BOTè‡ªèº«ã®ç™ºè¨€ã¯ 'assistant', ãã‚Œä»¥å¤–ã¯ 'user'
        role = "assistant" if msg["is_bot"] and msg["author_id"] == bot.user.id else "user"
        messages.append({"role": role, "content": msg["content"]})
    return messages

def build_gemini_chat_history(channel_id: int) -> list[dict]:
    """Gemini APIç”¨ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹"""
    history_deque = channel_data[channel_id]["history"]
    gemini_history = []
    for msg in history_deque:
        role = "model" if msg["is_bot"] and msg["author_id"] == bot.user.id else "user"
        # Geminiã¯'parts'ã®ä¸­ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚¹ãƒˆã¨ã—ã¦æŒã¤
        gemini_history.append({"role": role, "parts": [{"text": msg["content"]}]})
    return gemini_history


async def generate_response_stream(
    user_prompt: str, # "prompt" ã‹ã‚‰ "user_prompt" ã«å¤‰æ›´ (Geminiã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã®æ··åŒã‚’é¿ã‘ã‚‹)
    channel_id: int,
    message_to_edit: discord.Message,
    model_identifier: str, # "model" ã‹ã‚‰ "model_identifier" ã«å¤‰æ›´ (ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ããƒ¢ãƒ‡ãƒ«å)
) -> tuple[str | None, dict | None, str | None]: # (full_response, performance_metrics, error_message)

    model_type, actual_model_name = get_model_type_and_name(model_identifier)

    if not model_type or not actual_model_name:
        logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: å¿œç­”ç”Ÿæˆä¸å¯ - ãƒ¢ãƒ‡ãƒ«è­˜åˆ¥å­ãŒç„¡åŠ¹ ({model_identifier})")
        return None, None, f"ã‚¨ãƒ©ãƒ¼: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ« ({model_identifier}) ã®æŒ‡å®šãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚"

    channel_params = channel_data[channel_id]["params"]
    system_prompt_content = system_prompts.get(model_identifier, None) or DEFAULT_SYSTEM_PROMPT_TEXT
    current_prompt_display_name = get_prompt_name_from_content(system_prompts.get(model_identifier))

    full_response = ""
    last_update_time = time.monotonic()
    last_update_len = 0
    performance_metrics = None
    error_message = None
    stopped_by_user = False
    start_time = time.monotonic() # ç”Ÿæˆé–‹å§‹æ™‚é–“

    async def update_footer(status: str):
        if message_to_edit and message_to_edit.embeds:
            try:
                embed = message_to_edit.embeds[0]
                footer_text = f"Model: {actual_model_name} ({model_type}) | Prompt: {current_prompt_display_name} | {status}"
                embed.set_footer(text=footer_text)
                await message_to_edit.edit(embed=embed)
            except (discord.NotFound, discord.HTTPException):
                pass # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ¶ˆãˆãŸã‚Šç·¨é›†ã§ããªã„å ´åˆã¯ç„¡è¦–

    await update_footer("æ€è€ƒä¸­...")

    if model_type == "ollama":
        # --- Ollama APIå‘¼ã³å‡ºã— ---
        # /api/chat ã‚’ä½¿ã†ã®ã§ã€æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚‚å«ã‚ã¦messagesã«å«ã‚ã‚‹
        ollama_messages_context = build_ollama_chat_context(channel_id) 
        # ollama_messages_contextã®æœ€å¾Œã®è¦ç´ ãŒæœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¯ãšãªã®ã§ã€
        # generate_response_streamã«æ¸¡ã•ã‚ŒãŸ user_prompt ã¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèªã—ã¦ã‚‚ã‚ˆã„

        data = {
            "model": actual_model_name,
            "messages": ollama_messages_context, # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã‚€å…¨å±¥æ­´
            "system": system_prompt_content,
            "stream": True,
            "options": { 
                "temperature": channel_params.get("temperature"),
                "top_k": channel_params.get("top_k"),
                "top_p": channel_params.get("top_p"),
            }
        }
        data["options"] = {k: v for k, v in data["options"].items() if v is not None}
        if not data["system"]: del data["system"] # ç©ºãªã‚‰é€ã‚‰ãªã„

        api_url = f"{OLLAMA_API_URL}/api/chat" 
        logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Ollamaãƒ¢ãƒ‡ãƒ« '{actual_model_name}' (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {current_prompt_display_name}) ã«ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        response_key_ollama = "message" 
        content_key_ollama = "content"
        done_key_ollama = "done"

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(api_url, json=data, timeout=aiohttp.ClientTimeout(total=600)) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Ollama APIã‚¨ãƒ©ãƒ¼ ({response.status}): {error_text}")
                        return None, None, f"Ollama APIã‚¨ãƒ©ãƒ¼ ({response.status})ã€‚ã‚µãƒ¼ãƒãƒ¼ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

                    async for line in response.content:
                        if channel_data[channel_id]["stop_generation_requested"]:
                            logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«ã‚ˆã‚ŠOllamaå¿œç­”ç”Ÿæˆåœæ­¢")
                            stopped_by_user = True
                            error_message = "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šå¿œç­”ç”ŸæˆãŒåœæ­¢ã•ã‚Œã¾ã—ãŸã€‚"
                            break
                        if line:
                            try:
                                chunk = json.loads(line.decode('utf-8'))
                                chunk_response_content = ""

                                if response_key_ollama in chunk and isinstance(chunk[response_key_ollama], dict) and \
                                   content_key_ollama in chunk[response_key_ollama] and \
                                   not chunk.get(done_key_ollama, False): # APIä»•æ§˜ä¸Šdone=falseã§ã‚‚contentã¯æ¥ã‚‹
                                    chunk_response_content = chunk[response_key_ollama][content_key_ollama]

                                if chunk_response_content:
                                    full_response += chunk_response_content
                                    current_time = time.monotonic()
                                    if (current_time - last_update_time > STREAM_UPDATE_INTERVAL or
                                            len(full_response) - last_update_len > STREAM_UPDATE_CHARS):
                                        if message_to_edit and message_to_edit.embeds:
                                            display_response = full_response
                                            if len(display_response) > 4000:
                                                display_response = display_response[:4000] + "..."
                                            embed = message_to_edit.embeds[0]
                                            embed.description = display_response + " â–Œ"
                                            await update_footer("ç”Ÿæˆä¸­...")
                                            try:
                                                await message_to_edit.edit(embed=embed)
                                                last_update_time = current_time
                                                last_update_len = len(full_response)
                                            except discord.NotFound:
                                                logger.warning(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Ollamaã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç·¨é›†å¤±æ•— - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¶ˆå¤±")
                                                message_to_edit = None; error_message = "ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã«å†…éƒ¨ã‚¨ãƒ©ãƒ¼ (ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¶ˆå¤±)ã€‚"; break
                                            except discord.HTTPException as e:
                                                logger.warning(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Ollamaã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç·¨é›†å¤±æ•—: {e}")
                                
                                # /api/chat ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®æœ€å¾Œã¯ done: true ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å«ã‚€
                                if chunk.get(done_key_ollama, False): 
                                    end_time = time.monotonic()
                                    total_duration = end_time - start_time
                                    metrics_data = {
                                        "total_duration": total_duration, # ã“ã‚Œã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´è¨ˆæ¸¬ã®ç·æ™‚é–“
                                        "api_total_duration_sec": chunk.get('total_duration', 0) / 1e9 if chunk.get('total_duration') else 0, # APIãŒè¿”ã™ç·æ™‚é–“
                                        "load_duration_sec": chunk.get('load_duration', 0) / 1e9 if chunk.get('load_duration') else 0,
                                        "prompt_eval_count": chunk.get('prompt_eval_count', 0),
                                        "prompt_eval_duration_sec": chunk.get('prompt_eval_duration', 0) / 1e9 if chunk.get('prompt_eval_duration') else 0,
                                        "eval_count": chunk.get('eval_count', 0), # ã“ã‚ŒãŒç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«ç›¸å½“
                                        "eval_duration_sec": chunk.get('eval_duration', 0) / 1e9 if chunk.get('eval_duration') else 0 # ç”Ÿæˆæ™‚é–“
                                    }
                                    eval_duration = metrics_data["eval_duration_sec"]
                                    eval_count = metrics_data["eval_count"]
                                    if eval_duration > 0 and eval_count > 0:
                                        tps = eval_count / eval_duration
                                        metrics_data["tokens_per_second"] = tps if not math.isnan(tps) else 0.0
                                    else: # eval_durationãŒ0ã®å ´åˆã§ã‚‚ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ç·æ™‚é–“ã§æ¦‚ç®—TPSã‚’è¨ˆç®—
                                        if total_duration > 0 and eval_count > 0 :
                                            metrics_data["tokens_per_second"] = eval_count / total_duration
                                        else:
                                            metrics_data["tokens_per_second"] = 0.0

                                    metrics_data["total_tokens"] = eval_count if not math.isnan(eval_count) else 0 # ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
                                    performance_metrics = metrics_data
                                    logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Ollamaç”Ÿæˆå®Œäº† ({total_duration:.2f}s client / {metrics_data['api_total_duration_sec']:.2f}s api). ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {performance_metrics.get('tokens_per_second', 0):.2f} tok/s, {performance_metrics.get('total_tokens', 0)} tokens.")
                                    break
                            except json.JSONDecodeError as e:
                                logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Ollama JSONè§£æå¤±æ•—: {e}. Line: {line.decode('utf-8', errors='ignore')}")
                            except Exception as e:
                                logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Ollamaã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                                error_message = "Ollamaã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã€‚"; break
                    if stopped_by_user: performance_metrics = None
        except asyncio.TimeoutError:
            logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Ollama APIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            error_message = "Ollama API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚Ollamaã‚µãƒ¼ãƒãƒ¼ç¢ºèªè¦ã€‚"
        except aiohttp.ClientConnectorError as e:
            logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Ollama APIæ¥ç¶šå¤±æ•—: {e}")
            error_message = f"Ollama API ({OLLAMA_API_URL}) æ¥ç¶šä¸å¯ã€‚ã‚µãƒ¼ãƒãƒ¼ç¢ºèªè¦ã€‚"
        except Exception as e:
            logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Ollama APIãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸­ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            error_message = f"Ollamaå¿œç­”ç”Ÿæˆä¸­ã‚¨ãƒ©ãƒ¼: {str(e)}"

    elif model_type == "gemini" and genai:
        # --- Gemini APIå‘¼ã³å‡ºã— ---
        logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Geminiãƒ¢ãƒ‡ãƒ« '{actual_model_name}' (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {current_prompt_display_name}) ã«ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        try:
            gemini_model_obj = genai.GenerativeModel(
                model_name=actual_model_name, 
                system_instruction=system_prompt_content if system_prompt_content else None,
                safety_settings=GEMINI_SAFETY_SETTINGS 
            )
            
            # Geminiã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€æ–°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã¾ãªã„å½¢ã§æ§‹ç¯‰
            chat_history_for_gemini = build_gemini_chat_history(channel_id)
            # æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã®ã§ã€ãã‚Œã‚’å–ã‚Šé™¤ãã‹ã€
            # ã‚‚ã—ãã¯build_gemini_chat_historyãŒæœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã¾ãªã„ã‚ˆã†ã«ä¿®æ­£ã™ã‚‹ã€‚
            # ç¾çŠ¶ã®build_gemini_chat_historyã¯å…¨å±¥æ­´ã‚’ä½œã‚‹ã®ã§ã€æœ€å¾Œã®è¦ç´ ãŒç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã€‚
            # generate_contentã®contentsã«ã¯ã€[å±¥æ­´(ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãªã—), æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›]ã®å½¢ã‹ã€
            # [å…¨å±¥æ­´(ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚ã‚Š)]ã®ã©ã¡ã‚‰ã‹ã§æ¸¡ã™ã€‚SDKã¯å¾Œè€…ã‚’æ¨å¥¨ã€‚
            # ãã®ãŸã‚ã€build_gemini_chat_historyã§ä½œæˆã—ãŸå…¨å±¥æ­´ã‚’ãã®ã¾ã¾æ¸¡ã™ã€‚
            # ãŸã ã—ã€user_promptã¯åˆ¥ã§æ¸¡ã•ãªã„ã€‚
            
            contents_for_gemini = chat_history_for_gemini # build_gemini_chat_historyãŒæœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å«ã‚€

            generation_config_params = {
                "temperature": channel_params.get("temperature"),
                "top_k": channel_params.get("top_k"),
                "top_p": channel_params.get("top_p"),
            }
            generation_config_params = {k: v for k, v in generation_config_params.items() if v is not None}
            gemini_generation_config = genai.types.GenerationConfig(**generation_config_params) if generation_config_params else None

            # SDKã® generate_content ã¯ã€contents ã«ä¼šè©±å±¥æ­´å…¨ä½“ (æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚‚å«ã‚€) ã‚’æ¸¡ã™
            response_stream = await bot.loop.run_in_executor( 
                None,
                functools.partial(
                    gemini_model_obj.generate_content,
                    contents=contents_for_gemini, # ã“ã“ã«æœ€æ–°ã® user_prompt ã‚’å«ã‚€å…¨å±¥æ­´
                    stream=True,
                    generation_config=gemini_generation_config
                )
            )
            
            usage_metadata_final = None 

            for chunk in response_stream:
                if channel_data[channel_id]["stop_generation_requested"]:
                    logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«ã‚ˆã‚ŠGeminiå¿œç­”ç”Ÿæˆåœæ­¢")
                    stopped_by_user = True; error_message = "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šå¿œç­”ç”ŸæˆãŒåœæ­¢ã•ã‚Œã¾ã—ãŸã€‚"; break
                
                # chunk.text ãŒå­˜åœ¨ã—ã€å†…å®¹ãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç† (ç©ºã®ãƒãƒ£ãƒ³ã‚¯ãŒæ¥ã‚‹ã“ã¨ãŒã‚ã‚‹)
                if hasattr(chunk, 'text') and chunk.text:
                    full_response += chunk.text
                    current_time = time.monotonic()
                    if (current_time - last_update_time > STREAM_UPDATE_INTERVAL or
                            len(full_response) - last_update_len > STREAM_UPDATE_CHARS):
                        if message_to_edit and message_to_edit.embeds:
                            display_response = full_response
                            if len(display_response) > 4000: display_response = display_response[:4000] + "..."
                            embed = message_to_edit.embeds[0]
                            embed.description = display_response + " â–Œ"
                            await update_footer("ç”Ÿæˆä¸­...")
                            try:
                                await message_to_edit.edit(embed=embed)
                                last_update_time = current_time; last_update_len = len(full_response)
                            except discord.NotFound:
                                logger.warning(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Geminiã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç·¨é›†å¤±æ•— - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¶ˆå¤±")
                                message_to_edit = None; error_message = "ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã«å†…éƒ¨ã‚¨ãƒ©ãƒ¼ (ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¶ˆå¤±)ã€‚"; break
                            except discord.HTTPException as e:
                                logger.warning(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Geminiã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç·¨é›†å¤±æ•—: {e}")
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®æœ€å¾Œã«usage_metadataãŒå«ã¾ã‚Œã‚‹ã‹ç¢ºèª
                if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                    usage_metadata_final = chunk.usage_metadata # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ


            if not stopped_by_user: 
                end_time = time.monotonic()
                total_duration = end_time - start_time
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒ å®Œäº†å¾Œã€ãƒ¡ã‚¤ãƒ³ã®responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰usage_metadataã‚’å–å¾—
                # response_streamãŒã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã®å ´åˆã€é€šå¸¸ã¯ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†å¾Œã«æƒ…å ±ã‚’æŒã¤
                if not usage_metadata_final and hasattr(response_stream, 'usage_metadata'):
                    usage_metadata_final = response_stream.usage_metadata
                
                # ã‚‚ã—resolveãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Œã°è©¦ã™ (éåŒæœŸSDKã®ãƒ‘ã‚¿ãƒ¼ãƒ³)
                if not usage_metadata_final and hasattr(response_stream, 'resolve'):
                    try:
                        final_resolved_response = await bot.loop.run_in_executor(None, response_stream.resolve)
                        if hasattr(final_resolved_response, 'usage_metadata'):
                            usage_metadata_final = final_resolved_response.usage_metadata
                    except Exception as e_resolve:
                        logger.warning(f"Gemini ã‚¹ãƒˆãƒªãƒ¼ãƒ è§£æ±ºä¸­ã®ã‚¨ãƒ©ãƒ¼: {e_resolve}")


                prompt_tokens = 0
                candidate_tokens = 0 # ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ (Geminiã§ã¯candidates_token_count)

                if usage_metadata_final:
                    prompt_tokens = getattr(usage_metadata_final, 'prompt_token_count', 0)
                    candidate_tokens = getattr(usage_metadata_final, 'candidates_token_count', 0) 
                
                metrics_data = {
                    "total_duration": total_duration, # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´è¨ˆæ¸¬
                    "prompt_tokens": prompt_tokens,
                    "generated_tokens": candidate_tokens, 
                    "total_tokens": prompt_tokens + candidate_tokens, # Geminiã¯prompt+generatedã‚’totalã¨ã—ãªã„å ´åˆãŒã‚ã‚‹ã®ã§è‡ªå‰ã§è¨ˆç®—
                    "tokens_per_second": 0.0
                }
                if total_duration > 0 and candidate_tokens > 0:
                    tps = candidate_tokens / total_duration
                    metrics_data["tokens_per_second"] = tps if not math.isnan(tps) else 0.0
                
                performance_metrics = metrics_data
                logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Geminiç”Ÿæˆå®Œäº† ({total_duration:.2f}s). ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {performance_metrics.get('tokens_per_second',0):.2f} tok/s, {performance_metrics.get('generated_tokens',0)} gen tokens, {performance_metrics.get('prompt_tokens',0)} prompt tokens.")

        except google_exceptions.DeadlineExceeded as e:
            logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Gemini APIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {e}")
            error_message = "Gemini API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚"
        except google_exceptions.ResourceExhausted as e: 
            logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Gemini APIãƒªã‚½ãƒ¼ã‚¹ä¸Šé™è¶…é: {e}")
            error_message = f"Gemini API ãƒªã‚½ãƒ¼ã‚¹ä¸Šé™è¶…éã€‚ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚" # è©³ç´°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯çœç•¥
        except google_exceptions.InvalidArgument as e: 
            logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Gemini APIä¸æ­£ãªå¼•æ•°: {e}")
            error_message = f"Gemini API ã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¼•æ•°ãŒä¸æ­£ã§ã™ (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé•·ã™ãã‚‹ã€å½¢å¼ãŒé•ã†ç­‰)ã€‚"
        except google_exceptions.FailedPrecondition as e: # APIã‚­ãƒ¼ãŒç„¡åŠ¹ãªã©
            logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Gemini APIäº‹å‰æ¡ä»¶ã‚¨ãƒ©ãƒ¼: {e}")
            error_message = f"Gemini APIã®äº‹å‰æ¡ä»¶ã‚¨ãƒ©ãƒ¼ï¼ˆAPIã‚­ãƒ¼ãŒç„¡åŠ¹ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šä¸å‚™ãªã©ï¼‰ã€‚"
        except google_exceptions.GoogleAPIError as e: # ãã®ä»–ã®Google APIã‚¨ãƒ©ãƒ¼
            logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Gemini APIã‚¨ãƒ©ãƒ¼: {e}")
            error_message = f"Gemini APIã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
        except Exception as e:
            logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Gemini APIãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸­äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            error_message = f"Geminiå¿œç­”ç”Ÿæˆä¸­ã‚¨ãƒ©ãƒ¼: {str(e)}"

        if stopped_by_user: performance_metrics = None

    else: 
        logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: æœªçŸ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ— '{model_type}' ã¾ãŸã¯Gemini APIãŒç„¡åŠ¹ã§ã™ã€‚")
        error_message = f"ã‚¨ãƒ©ãƒ¼: æœªçŸ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ— '{model_type}'ã€ã¾ãŸã¯è©²å½“APIãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"

    return full_response.strip(), performance_metrics, error_message


# --- å®šæœŸå®Ÿè¡Œã‚¿ã‚¹ã‚¯ ---
@tasks.loop(minutes=PROMPT_RELOAD_INTERVAL_MINUTES)
async def reload_prompts_task():
    global available_prompts
    logger.info("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šæœŸãƒªãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ...")
    try:
        new_prompts = await bot.loop.run_in_executor(
            None,
            functools.partial(_load_prompts_sync, prompts_dir_path)
        )
        if new_prompts != available_prompts:
            added = list(set(new_prompts.keys()) - set(available_prompts.keys()))
            removed = list(set(available_prompts.keys()) - set(new_prompts.keys()))
            updated = [k for k, v in new_prompts.items() if k in available_prompts and available_prompts[k] != v]
            available_prompts = new_prompts
            log_msg = f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒªã‚¹ãƒˆæ›´æ–° ({len(available_prompts)}å€‹)ã€‚"
            if added: log_msg += f" è¿½åŠ : {added}"
            if removed: log_msg += f" å‰Šé™¤: {removed}"
            if updated: log_msg += f" æ›´æ–°: {updated}"
            logger.info(log_msg)
        else:
            logger.info("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¤‰æ›´ãªã—ã€‚")
    except Exception as e:
        logger.error(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒªãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

@reload_prompts_task.before_loop
async def before_reload_prompts():
    await bot.wait_until_ready()
    logger.info(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒªãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯æº–å‚™å®Œäº† ({PROMPT_RELOAD_INTERVAL_MINUTES}åˆ†ã”ã¨)ã€‚")
    # on_ready ã§åˆå›å®Ÿè¡Œã™ã‚‹ã®ã§ã“ã“ã§ã¯ä¸è¦
    # await reload_prompts_task() 

@tasks.loop(minutes=MODEL_UPDATE_INTERVAL_MINUTES)
async def update_models_task():
    logger.info("ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆå®šæœŸæ›´æ–°å®Ÿè¡Œ...")
    try:
        await fetch_and_update_available_models() # æ–°ã—ã„é–¢æ•°ã§ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’æ›´æ–°
    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆæ›´æ–°ã‚¿ã‚¹ã‚¯ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

@update_models_task.before_loop
async def before_update_models():
    await bot.wait_until_ready()
    logger.info(f"ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆæ›´æ–°ã‚¿ã‚¹ã‚¯æº–å‚™å®Œäº† ({MODEL_UPDATE_INTERVAL_MINUTES}åˆ†ã”ã¨)ã€‚")
    # on_ready ã§åˆå›å®Ÿè¡Œã™ã‚‹ã®ã§ã“ã“ã§ã¯ä¸è¦
    # await update_models_task() 


# --- Discord ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ© ---
@bot.event
async def on_ready():
    global active_model, available_bot_models, available_prompts

    logger.info(f'{bot.user} (ID: {bot.user.id}) ã¨ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã—ãŸ')

    if not TOKEN: logger.critical("ç’°å¢ƒå¤‰æ•° 'DISCORD_TOKEN' æœªè¨­å®šã€‚"); sys.exit(1)
    if CHAT_CHANNEL_ID is None: logger.critical("ç’°å¢ƒå¤‰æ•° 'CHAT_CHANNEL_ID' ç„¡åŠ¹ã€‚"); sys.exit(1)
    if genai is None and GEMINI_API_KEY : 
        logger.warning("Gemini APIã‚­ãƒ¼ã¯è¨­å®šã•ã‚Œã¦ã„ã¾ã™ãŒã€'google-generativeai'ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ãŸãŸã‚ã€Geminiæ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
    elif genai is None and not GEMINI_API_KEY: 
         logger.info("Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ã€ã¾ãŸã¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚ã‚Šã¾ã›ã‚“ã€‚Geminiæ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ã€‚")
    if HISTORY_LIMIT < 1:
        logger.critical(f"ç’°å¢ƒå¤‰æ•° 'HISTORY_LIMIT' ã®å€¤ ({HISTORY_LIMIT}) ãŒä¸æ­£ã§ã™ã€‚1ä»¥ä¸Šã®å€¤ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚BOTã®å‹•ä½œã«æ·±åˆ»ãªæ”¯éšœãŒå‡ºã¾ã™ã€‚")
    # åˆå›ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆå–å¾—
    logger.info("åˆå›ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆå–å¾—é–‹å§‹...")
    await fetch_and_update_available_models() 

    # åˆå›ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿
    try:
        available_prompts = await bot.loop.run_in_executor(
            None,
            functools.partial(_load_prompts_sync, prompts_dir_path)
        )
        logger.info(f"åˆå›ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿å®Œäº† ({len(available_prompts)}å€‹): {list(available_prompts.keys())}")
    except Exception as e:
        logger.error(f"åˆå›ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)


    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã®æ±ºå®šã¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
    if not active_model: 
        logger.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«(.env)æœªè¨­å®šã€‚åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‹ã‚‰è‡ªå‹•é¸æŠè©¦è¡Œ...")
        if available_bot_models:
            # å„ªå…ˆé †ä½: 1. Ollamaãƒ¢ãƒ‡ãƒ«, 2. Gemini 2.5ç³», 3. ãã®ä»–Gemini
            ollama_models_on_ready = [m for m in available_bot_models if m.startswith("ollama:")]
            gemini_2_5_on_ready = [m for m in available_bot_models if m.startswith("gemini:") and "2.5" in m]
            
            if ollama_models_on_ready: active_model = ollama_models_on_ready[0]
            elif gemini_2_5_on_ready: active_model = gemini_2_5_on_ready[0]
            else: active_model = available_bot_models[0] # ä¸Šè¨˜ãŒãªã‘ã‚Œã°ãƒªã‚¹ãƒˆã®å…ˆé ­
            logger.info(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ‡ãƒ«ã‚’ '{active_model}' ã«è¨­å®šã—ã¾ã—ãŸã€‚")
        else:
            logger.error("åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚`/model` ã‚³ãƒãƒ³ãƒ‰ã§æ‰‹å‹•è¨­å®šãŒå¿…è¦ã§ã™ã€‚")
    elif active_model not in available_bot_models: 
        logger.warning(f"ç’°å¢ƒå¤‰æ•°ã§æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ« '{active_model}' ã¯åˆ©ç”¨å¯èƒ½ãƒªã‚¹ãƒˆã«ã‚ã‚Šã¾ã›ã‚“ã€‚")
        if available_bot_models:
            logger.info(f"ä»£ã‚ã‚Šã«ãƒªã‚¹ãƒˆã®å…ˆé ­ãƒ¢ãƒ‡ãƒ« '{available_bot_models[0]}' ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            active_model = available_bot_models[0]
        else:
            logger.error("åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒãƒªã‚¹ãƒˆã«ãªã„ãŸã‚ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã§ãã¾ã›ã‚“ã€‚")
            active_model = None 
    else: # DEFAULT_MODELãŒè¨­å®šã•ã‚Œã€ã‹ã¤ãƒªã‚¹ãƒˆã«ã‚‚å­˜åœ¨ã™ã‚‹å ´åˆ
        logger.info(f"ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ« '{active_model}' ã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ‡ãƒ«ã«è¨­å®šã—ã¾ã—ãŸã€‚")


    if active_model and active_model not in system_prompts:
        system_prompts[active_model] = None 

    chat_channel = bot.get_channel(CHAT_CHANNEL_ID)
    if chat_channel and isinstance(chat_channel, discord.TextChannel):
        logger.info(f"ãƒãƒ£ãƒƒãƒˆãƒãƒ£ãƒ³ãƒãƒ« '{chat_channel.name}' (ID: {CHAT_CHANNEL_ID}) èªè­˜ã€‚å±¥æ­´èª­ã¿è¾¼ã¿é–‹å§‹...")
        await fetch_channel_history(chat_channel, limit=HISTORY_LIMIT * 2) 
    else:
        logger.error(f"æŒ‡å®šãƒãƒ£ãƒƒãƒˆãƒãƒ£ãƒ³ãƒãƒ«ID ({CHAT_CHANNEL_ID}) ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ãƒãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    try:
        synced = await bot.tree.sync()
        logger.info(f'{len(synced)}å€‹ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚³ãƒãƒ³ãƒ‰åŒæœŸå®Œäº†: {[cmd.name for cmd in synced]}')
    except Exception as e:
        logger.error(f"ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚³ãƒãƒ³ãƒ‰åŒæœŸã‚¨ãƒ©ãƒ¼: {e}")

    # ã‚¿ã‚¹ã‚¯ã®é–‹å§‹ã¯ on_ready ã§è¡Œã†
    if not reload_prompts_task.is_running(): 
        await before_reload_prompts() # before_loop ã‚’å‘¼ã³å‡ºã—ã¦ã‹ã‚‰é–‹å§‹
        reload_prompts_task.start()
    if not update_models_task.is_running(): 
        await before_update_models() # before_loop ã‚’å‘¼ã³å‡ºã—ã¦ã‹ã‚‰é–‹å§‹
        update_models_task.start()

    logger.info("BOTã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")


@bot.event
async def on_message(message: discord.Message):
    if message.author == bot.user or message.channel.id != CHAT_CHANNEL_ID or \
       message.content.startswith('/') or message.content.startswith(bot.command_prefix or ' unlikely_prefix '):
        return

    channel_id = message.channel.id

    if channel_data[channel_id]["is_generating"]:
        try:
            await message.reply("â³ ä»–ã®å¿œç­”ã‚’ç”Ÿæˆä¸­ã§ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚", mention_author=False, delete_after=10)
        except discord.HTTPException: pass
        logger.warning(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: å¿œç­”ç”Ÿæˆä¸­ã«æ–°è¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡ã€ã‚¹ã‚­ãƒƒãƒ—: {message.content[:50]}...")
        return

    current_active_model = active_model 
    if not current_active_model:
        try:
            await message.reply("âš ï¸ ãƒ¢ãƒ‡ãƒ«æœªé¸æŠã€‚`/model` ã‚³ãƒãƒ³ãƒ‰ã§é¸æŠã—ã¦ãã ã•ã„ã€‚", mention_author=False)
        except discord.HTTPException: pass
        return

    model_type, _ = get_model_type_and_name(current_active_model)
    if model_type == "gemini" and not genai:
        try:
            await message.reply("âš ï¸ ç¾åœ¨é¸æŠä¸­ã®ãƒ¢ãƒ‡ãƒ«ã¯Geminiãƒ¢ãƒ‡ãƒ«ã§ã™ãŒã€Gemini APIãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç®¡ç†è€…ã«é€£çµ¡ã™ã‚‹ã‹ã€åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚", mention_author=False)
        except discord.HTTPException: pass
        logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: Geminiãƒ¢ãƒ‡ãƒ« '{current_active_model}' ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã™ãŒã€Gemini APIã¯ç„¡åŠ¹ã§ã™ã€‚")
        return


    user_message_data = {
        "author_name": message.author.display_name, "author_id": message.author.id,
        "content": message.content, "timestamp": message.created_at.isoformat(), "is_bot": False
    }
    channel_data[channel_id]["history"].append(user_message_data)
    
    reply_message = None
    final_response_text = None
    metrics = None
    error_msg_from_generation = None 

    try:
        channel_data[channel_id]["is_generating"] = True
        channel_data[channel_id]["stop_generation_requested"] = False

        model_type_disp, model_name_disp = get_model_type_and_name(current_active_model)
        prompt_name_disp = get_prompt_name_from_content(system_prompts.get(current_active_model))
        
        placeholder_embed = Embed(description="æ€è€ƒä¸­... ğŸ¤”", color=discord.Color.light_gray())
        placeholder_embed.set_footer(text=f"Model: {model_name_disp} ({model_type_disp}) | Prompt: {prompt_name_disp}")
        reply_message = await message.reply(embed=placeholder_embed, mention_author=False)

        final_response_text, metrics, error_msg_from_generation = await generate_response_stream(
            user_prompt=message.content, # generate_response_streamå´ã§å±¥æ­´ã«å«ã‚ã‚‹ã‹åˆ¤æ–­
            channel_id=channel_id,
            message_to_edit=reply_message,
            model_identifier=current_active_model
        )

    except discord.HTTPException as e:
        logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†/ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
        error_msg_from_generation = f"å‡¦ç†ä¸­ã«Discord APIã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ:\n```\n{e}\n```"
    except Exception as e:
        logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ä¸­äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        error_msg_from_generation = "å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    finally:
        channel_data[channel_id]["is_generating"] = False
        channel_data[channel_id]["stop_generation_requested"] = False
        
    if reply_message:
        try:
            final_embed = reply_message.embeds[0] if reply_message.embeds else Embed()
            model_type_final, model_name_final = get_model_type_and_name(current_active_model)
            prompt_name_final = get_prompt_name_from_content(system_prompts.get(current_active_model))

            if error_msg_from_generation:
                if "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šå¿œç­”ç”ŸæˆãŒåœæ­¢ã•ã‚Œã¾ã—ãŸ" in error_msg_from_generation:
                    final_embed.title = "â¹ï¸ åœæ­¢"
                    stopped_text = final_response_text if final_response_text else '(å¿œç­”ãªã—)'
                    if len(stopped_text) > 3900: stopped_text = stopped_text[:3900] + "...(é€”ä¸­çœç•¥)" 
                    final_embed.description = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«ã‚ˆã‚Šå¿œç­”åœæ­¢ã€‚\n\n**ç”Ÿæˆé€”ä¸­å†…å®¹:**\n{stopped_text}"
                    final_embed.color = discord.Color.orange()
                else:
                    final_embed.title = "âš ï¸ ã‚¨ãƒ©ãƒ¼"
                    final_embed.description = f"å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼:\n\n{error_msg_from_generation}"
                    final_embed.color = discord.Color.red()
                final_embed.set_footer(text=f"Model: {model_name_final} ({model_type_final}) | Prompt: {prompt_name_final}")

            elif final_response_text is not None:
                final_embed.title = None
                display_final_text = final_response_text
                if len(display_final_text) > 4000: display_final_text = display_final_text[:4000] + "\n...(æ–‡å­—æ•°ä¸Šé™)"
                final_embed.description = display_final_text if display_final_text else "(ç©ºã®å¿œç­”)"
                final_embed.color = discord.Color.blue()

                footer_text_parts = [f"Model: {model_name_final} ({model_type_final})", f"Prompt: {prompt_name_final}"]
                if metrics:
                    channel_data[channel_id]["stats"].append(metrics) 
                    duration = metrics.get("total_duration", 0)
                    if duration > 0: footer_text_parts.append(f"{duration:.2f}s")

                    if model_type_final == "ollama":
                        tok_sec = metrics.get("tokens_per_second", 0)
                        total_tok = metrics.get("total_tokens", 0) # Ollamaã§ã¯eval_count (ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°)
                        if tok_sec > 0: footer_text_parts.append(f"{tok_sec:.2f} tok/s")
                        if total_tok > 0: footer_text_parts.append(f"{int(total_tok)} genTk")
                        # prompt_eval_count ã‚‚è¡¨ç¤ºã™ã‚‹ãªã‚‰è¿½åŠ 
                        # p_eval_c = metrics.get("prompt_eval_count", 0)
                        # if p_eval_c > 0: footer_text_parts.append(f"{int(p_eval_c)} prmTk")
                    elif model_type_final == "gemini":
                        tok_sec = metrics.get("tokens_per_second", 0)
                        gen_tok = metrics.get("generated_tokens", 0)
                        p_tok = metrics.get("prompt_tokens", 0)
                        if tok_sec > 0: footer_text_parts.append(f"{tok_sec:.2f} tok/s")
                        if gen_tok > 0: footer_text_parts.append(f"{int(gen_tok)} genTk")
                        if p_tok > 0: footer_text_parts.append(f"{int(p_tok)} prmTk")
                
                final_embed.set_footer(text=" | ".join(footer_text_parts))

                bot_message_data = {
                    "author_name": bot.user.display_name, "author_id": bot.user.id,
                    "content": final_response_text, 
                    "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat(), "is_bot": True
                }
                channel_data[channel_id]["history"].append(bot_message_data)
            else: 
                final_embed.title = "â“ ç„¡å¿œç­”"
                final_embed.description = "å¿œç­”ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚å…¥åŠ›å†…å®¹ã‚’ç¢ºèªã™ã‚‹ã‹ã€ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¤‰æ›´ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚"
                final_embed.color = discord.Color.dark_orange()
                final_embed.set_footer(text=f"Model: {model_name_final} ({model_type_final}) | Prompt: {prompt_name_final}")
                logger.warning(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‚‚ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚‚ãªã—ã€‚ model: {current_active_model}")
            
            await reply_message.edit(embed=final_embed)

        except discord.NotFound:
            logger.warning(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: æœ€çµ‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç·¨é›†å¤±æ•— - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¶ˆå¤± (ID: {reply_message.id})")
        except discord.HTTPException as e:
            logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: æœ€çµ‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç·¨é›†å¤±æ•—: {e}")
            err_code = getattr(e, 'code', 'N/A'); err_text = str(e)[:100]
            try: await message.channel.send(f"ã‚¨ãƒ©ãƒ¼: å¿œç­”æœ€çµ‚è¡¨ç¤ºå¤±æ•— (Code: {err_code}) - {err_text}", reference=message, mention_author=False)
            except discord.HTTPException: pass
        except IndexError: 
            logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: æœ€çµ‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç·¨é›†å¤±æ•— - Embedãªã—")
            try: await reply_message.edit(content="ã‚¨ãƒ©ãƒ¼: å¿œç­”è¡¨ç¤ºæº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸã€‚", embed=None)
            except discord.HTTPException: pass
        except Exception as e:
            logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: æœ€çµ‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç·¨é›†ä¸­äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try: await reply_message.edit(content="ã‚¨ãƒ©ãƒ¼: å¿œç­”ã®æœ€çµ‚è¡¨ç¤ºä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", embed=None)
            except discord.HTTPException: pass


# --- ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚³ãƒãƒ³ãƒ‰ ---
# --- ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆ ---#
async def model_autocomplete(interaction: discord.Interaction, current: str) -> list[app_commands.Choice[str]]:
    ollama_models = []
    gemini_2_5_models = []
    gemini_2_0_models = []
    gemini_1_5_models = []
    gemini_1_0_models = []
    gemini_gemma_models = []  # GeminiãŒãƒ›ã‚¹ãƒˆã™ã‚‹Gemmaãƒ¢ãƒ‡ãƒ«ç”¨
    other_gemini_models = []
    unknown_prefix_models = []

    # ãƒ¢ãƒ‡ãƒ«ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡
    # available_bot_models ã¯æ—¢ã«ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ã«ã‚½ãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™
    for model_id in available_bot_models:
        if model_id.startswith("ollama:"):
            ollama_models.append(model_id)
        elif model_id.startswith("gemini:"):
            # "gemini:" ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤ã„ãŸãƒ¢ãƒ‡ãƒ«åéƒ¨åˆ†ã‚’å–å¾—
            name_after_prefix = model_id[len("gemini:"):] 

            if name_after_prefix.startswith("gemini-2.5"):
                gemini_2_5_models.append(model_id)
            elif name_after_prefix.startswith("gemini-2.0"):
                gemini_2_0_models.append(model_id)
            elif name_after_prefix.startswith("gemini-1.5"):
                gemini_1_5_models.append(model_id)
            elif name_after_prefix.startswith("gemini-1.0"):
                gemini_1_0_models.append(model_id)
            elif name_after_prefix.startswith("gemma"): #ä¾‹: "gemini:gemma-3-12b-it"
                gemini_gemma_models.append(model_id)
            else:
                other_gemini_models.append(model_id)
        else:
            unknown_prefix_models.append(model_id)

    # æŒ‡å®šã•ã‚ŒãŸå„ªå…ˆé †ä½ã§ãƒªã‚¹ãƒˆã‚’çµåˆ
    # å„ã‚«ãƒ†ã‚´ãƒªå†…ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€å…ƒã®ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ã‚’ç¶­æŒã—ã¾ã™
    prioritized_model_list = (
        ollama_models +          # 1. Ollama
        gemini_2_5_models +      # 2. Gemini 2.5ç³»
        gemini_2_0_models +      # 3. Gemini 2.0ç³»
        gemini_1_5_models +      # 4. Gemini 1.5ç³»
        gemini_1_0_models +      # 5. Gemini 1.0ç³»
        gemini_gemma_models +    # 6. Gemini Gemmaç³»
        other_gemini_models +    # 7. ãã®ä»–ã®Geminiãƒ¢ãƒ‡ãƒ«
        unknown_prefix_models    # 8. æœªçŸ¥ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ (é€šå¸¸ã¯ç©º)
    )

    choices = []
    current_lower = current.lower()

    for model_id in prioritized_model_list:
        if current_lower in model_id.lower():
            choices.append(app_commands.Choice(name=model_id, value=model_id))
        if len(choices) >= 25: # Discordã®è¡¨ç¤ºä¸Šé™
            break
            
    return choices

async def prompt_autocomplete(interaction: discord.Interaction, current: str) -> list[app_commands.Choice[str]]:
    choices = []
    if current.lower() in PROMPT_NAME_DEFAULT.lower():
        choices.append(app_commands.Choice(name=PROMPT_NAME_DEFAULT, value=PROMPT_NAME_DEFAULT))
    
    custom_choices = [
        app_commands.Choice(name=name, value=name)
        for name in sorted(available_prompts.keys()) if current.lower() in name.lower()
    ]
    choices.extend(custom_choices)
    return choices[:25] # ã“ã¡ã‚‰ã‚‚25ä»¶ä¸Šé™

# --- ã‚³ãƒãƒ³ãƒ‰æœ¬ä½“ ---
@bot.tree.command(name="stop", description="ç¾åœ¨ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã§ç”Ÿæˆä¸­ã®AIã®å¿œç­”ã‚’åœæ­¢ã—ã¾ã™ã€‚")
async def stop_generation(interaction: discord.Interaction):
    channel_id = interaction.channel_id
    if channel_id != CHAT_CHANNEL_ID:
        await interaction.response.send_message("ã“ã®ã‚³ãƒãƒ³ãƒ‰ã¯æŒ‡å®šã•ã‚ŒãŸãƒãƒ£ãƒƒãƒˆãƒãƒ£ãƒ³ãƒãƒ«ã§ã®ã¿ä½¿ç”¨ã§ãã¾ã™ã€‚", ephemeral=True); return

    if channel_data[channel_id]["is_generating"]:
        if not channel_data[channel_id]["stop_generation_requested"]:
            channel_data[channel_id]["stop_generation_requested"] = True
            logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: ãƒ¦ãƒ¼ã‚¶ãƒ¼ {interaction.user} (ID: {interaction.user.id}) ã«ã‚ˆã‚Šåœæ­¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€‚")
            await interaction.response.send_message("â¹ï¸ å¿œç­”ã®åœæ­¢ã‚’è©¦ã¿ã¦ã„ã¾ã™...", ephemeral=True)
            try:
                if interaction.channel: await interaction.channel.send(f"âš ï¸ {interaction.user.mention} ãŒå¿œç­”ç”Ÿæˆã®åœæ­¢ã‚’è©¦ã¿ã¦ã„ã¾ã™ã€‚")
            except discord.HTTPException as e: logger.warning(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: åœæ­¢è©¦è¡Œã®å…¬é–‹ãƒ­ã‚°é€ä¿¡å¤±æ•—: {e}")
        else:
            await interaction.response.send_message("â„¹ï¸ æ—¢ã«åœæ­¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒé€ä¿¡ã•ã‚Œã¦ã„ã¾ã™ã€‚", ephemeral=True)
    else:
        await interaction.response.send_message("â„¹ï¸ ç¾åœ¨ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã§ç”Ÿæˆä¸­ã®å¿œç­”ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚", ephemeral=True)


@bot.tree.command(name="model", description="ä½¿ç”¨ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã¨ã€ãã®ãƒ¢ãƒ‡ãƒ«ç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®šã—ã¾ã™ã€‚")
@app_commands.describe(
    model_identifier="ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ (ä¾‹: ollama:llama3, gemini:gemini-1.5-pro-latest)",
    prompt_name=f"é©ç”¨ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ('{PROMPT_DIR_NAME}'å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã€ã¾ãŸã¯'{PROMPT_NAME_DEFAULT}')"
)
@app_commands.autocomplete(model_identifier=model_autocomplete, prompt_name=prompt_autocomplete)
async def select_model(interaction: discord.Interaction, model_identifier: str, prompt_name: str = None):
    global active_model 
    channel_id = interaction.channel_id
    if channel_id != CHAT_CHANNEL_ID:
        await interaction.response.send_message("ã“ã®ã‚³ãƒãƒ³ãƒ‰ã¯æŒ‡å®šã•ã‚ŒãŸãƒãƒ£ãƒƒãƒˆãƒãƒ£ãƒ³ãƒãƒ«ã§ã®ã¿ä½¿ç”¨ã§ãã¾ã™ã€‚", ephemeral=True); return
    
    await interaction.response.defer(ephemeral=True, thinking=False)

    sel_model_type, sel_model_name = get_model_type_and_name(model_identifier)
    if sel_model_type == "gemini" and not genai:
        await interaction.followup.send(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ« '{model_identifier}' ã¯Geminiãƒ¢ãƒ‡ãƒ«ã§ã™ãŒã€Gemini APIãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", ephemeral=True)
        return

    if model_identifier not in available_bot_models:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå¤ã„å¯èƒ½æ€§ã‚‚ã‚ã‚‹ã®ã§ã€å†å–å¾—ã‚’è©¦ã¿ã‚‹
        await fetch_and_update_available_models()
        if model_identifier not in available_bot_models: # å†å–å¾—ã—ã¦ã‚‚ãªã„å ´åˆ
            model_list_str = "\n- ".join(available_bot_models) if available_bot_models else "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç®¡ç†è€…ã«é€£çµ¡ã—ã¦ãã ã•ã„ã€‚"
            await interaction.followup.send(
                f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ« '{model_identifier}' ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°å¾Œã‚‚)ã€‚\nåˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ« (ã‚­ãƒ£ãƒƒã‚·ãƒ¥):\n- {model_list_str}",
                ephemeral=True
            ); return

    previous_model_identifier = active_model
    previous_prompt_content = system_prompts.get(previous_model_identifier)
    prev_model_type, prev_model_name = get_model_type_and_name(previous_model_identifier)
    previous_prompt_name_display = get_prompt_name_from_content(previous_prompt_content)

    active_model = model_identifier 
    model_changed = previous_model_identifier != active_model
    
    prompt_actually_changed = False
    ephemeral_message_lines = []
    current_model_type_disp, current_model_name_disp = get_model_type_and_name(active_model)


    if prompt_name: 
        new_prompt_content_for_selected_model: str | None = None
        valid_prompt_selection = False

        if prompt_name == PROMPT_NAME_DEFAULT:
            new_prompt_content_for_selected_model = None 
            valid_prompt_selection = True
        elif prompt_name in available_prompts:
            new_prompt_content_for_selected_model = available_prompts[prompt_name]
            valid_prompt_selection = True
        else:
            ephemeral_message_lines.append(f"âš ï¸ ä¸æ˜ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå '{prompt_name}'ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯å¤‰æ›´ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            current_prompt_for_active_model = system_prompts.get(active_model)
            prompt_name_to_display = get_prompt_name_from_content(current_prompt_for_active_model)
            ephemeral_message_lines.append(f"ğŸ“„ ãƒ¢ãƒ‡ãƒ« **{current_model_name_disp} ({current_model_type_disp})** ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ **{prompt_name_to_display}** ã®ã¾ã¾ã§ã™ã€‚")


        if valid_prompt_selection:
            current_prompt_for_active_model = system_prompts.get(active_model)
            if new_prompt_content_for_selected_model != current_prompt_for_active_model:
                system_prompts[active_model] = new_prompt_content_for_selected_model
                prompt_actually_changed = True
                ephemeral_message_lines.append(f"ğŸ“„ ãƒ¢ãƒ‡ãƒ« **{current_model_name_disp} ({current_model_type_disp})** ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ **{prompt_name}** ã«è¨­å®šã—ã¾ã—ãŸã€‚")
                logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: ãƒ¢ãƒ‡ãƒ« '{active_model}' ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š -> '{prompt_name}' by {interaction.user}")
            else:
                ephemeral_message_lines.append(f"â„¹ï¸ ãƒ¢ãƒ‡ãƒ« **{current_model_name_disp} ({current_model_type_disp})** ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯æ—¢ã« **{prompt_name}** ã§ã™ã€‚")
    else: 
        if active_model not in system_prompts: 
            system_prompts[active_model] = None 
            prompt_actually_changed = True # æš—é»™çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«å¤‰æ›´ã•ã‚ŒãŸ (ãƒ¢ãƒ‡ãƒ«å¤‰æ›´æ™‚ãªã©)
            ephemeral_message_lines.append(f"ğŸ“„ ãƒ¢ãƒ‡ãƒ« **{current_model_name_disp} ({current_model_type_disp})** ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœªè¨­å®šã ã£ãŸãŸã‚ã€**{PROMPT_NAME_DEFAULT}** ã‚’è¨­å®šã€‚")
        else: 
            maintained_prompt_content = system_prompts.get(active_model)
            maintained_prompt_name = get_prompt_name_from_content(maintained_prompt_content)
            ephemeral_message_lines.append(f"â„¹ï¸ ãƒ¢ãƒ‡ãƒ« **{current_model_name_disp} ({current_model_type_disp})** ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ **{maintained_prompt_name}** ã‚’ç¶­æŒã€‚")


    final_ephemeral_message = []
    if model_changed:
        final_ephemeral_message.append(f"âœ… ãƒ¢ãƒ‡ãƒ«å¤‰æ›´: **{prev_model_name or 'N/A'} ({prev_model_type or 'N/A'})** â†’ **{current_model_name_disp} ({current_model_type_disp})**ã€‚")
        logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ‡ãƒ«å¤‰æ›´ -> '{active_model}' by {interaction.user}")
    else:
        final_ephemeral_message.append(f"â„¹ï¸ ãƒ¢ãƒ‡ãƒ«ã¯ **{current_model_name_disp} ({current_model_type_disp})** ã®ã¾ã¾ã€‚")
    
    final_ephemeral_message.extend(ephemeral_message_lines)
    await interaction.followup.send("\n".join(final_ephemeral_message), ephemeral=True)

    if model_changed or prompt_actually_changed:
        log_parts = []
        final_active_prompt_name = get_prompt_name_from_content(system_prompts.get(active_model))
        
        if model_changed:
            log_parts.append(f"ãƒ¢ãƒ‡ãƒ«: **{prev_model_name or 'N/A'} ({prev_model_type or 'N/A'})** â†’ **{current_model_name_disp} ({current_model_type_disp})**")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå®Ÿéš›ã«å¤‰ã‚ã£ãŸã‹ã€ã‚ã‚‹ã„ã¯ãƒ¢ãƒ‡ãƒ«ãŒå¤‰ã‚ã£ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæš—é»™çš„ã«ç¶­æŒ/è¨­å®šã•ã‚ŒãŸå ´åˆã«è¡¨ç¤º
        if prompt_actually_changed:
            log_parts.append(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ({current_model_name_disp}): **{previous_prompt_name_display if model_changed else previous_prompt_name_display}** â†’ **{final_active_prompt_name}**")
        elif model_changed: # ãƒ¢ãƒ‡ãƒ«ã¯å¤‰ã‚ã£ãŸãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯æ˜ç¤ºçš„ã«å¤‰æ›´ã•ã‚Œãšã€ç¶­æŒã•ã‚ŒãŸå ´åˆ
             log_parts.append(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ({current_model_name_disp}): **{final_active_prompt_name}** (ç¶­æŒ)")


        if log_parts:
            public_log_message = f"ğŸ”§ {interaction.user.mention} ãŒè¨­å®šå¤‰æ›´: " + ", ".join(log_parts)
            try:
                if interaction.channel: await interaction.channel.send(public_log_message)
            except discord.HTTPException as e: logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: ãƒ¢ãƒ‡ãƒ«/ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¤‰æ›´å…¬é–‹ãƒ­ã‚°é€ä¿¡å¤±æ•—: {e}")


@bot.tree.command(name="set_prompt", description="ç¾åœ¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ¢ãƒ‡ãƒ«ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®šã—ã¾ã™ã€‚")
@app_commands.describe(prompt_name=f"é©ç”¨ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ('{PROMPT_DIR_NAME}'å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã€ã¾ãŸã¯'{PROMPT_NAME_DEFAULT}')")
@app_commands.autocomplete(prompt_name=prompt_autocomplete)
async def set_prompt(interaction: discord.Interaction, prompt_name: str):
    channel_id = interaction.channel_id
    if channel_id != CHAT_CHANNEL_ID:
        await interaction.response.send_message("ã“ã®ã‚³ãƒãƒ³ãƒ‰ã¯æŒ‡å®šã•ã‚ŒãŸãƒãƒ£ãƒƒãƒˆãƒãƒ£ãƒ³ãƒãƒ«ã§ã®ã¿ä½¿ç”¨ã§ãã¾ã™ã€‚", ephemeral=True); return
    if not active_model:
        await interaction.response.send_message("âš ï¸ ãƒ¢ãƒ‡ãƒ«æœªé¸æŠã€‚`/model` ã‚³ãƒãƒ³ãƒ‰ã§é¸æŠã—ã¦ãã ã•ã„ã€‚", ephemeral=True); return
    
    await interaction.response.defer(ephemeral=True, thinking=False)

    current_prompt_content = system_prompts.get(active_model)
    current_prompt_name_display = get_prompt_name_from_content(current_prompt_content)
    new_prompt_content: str | None = None
    valid_prompt = False

    if prompt_name == PROMPT_NAME_DEFAULT:
        new_prompt_content = None
        valid_prompt = True
    elif prompt_name in available_prompts:
        new_prompt_content = available_prompts[prompt_name]
        valid_prompt = True
    else:
        await interaction.followup.send(f"âŒ ã‚¨ãƒ©ãƒ¼: ä¸æ˜ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå '{prompt_name}'ã€‚", ephemeral=True); return

    if valid_prompt:
        if new_prompt_content != current_prompt_content:
            system_prompts[active_model] = new_prompt_content
            model_type_disp, model_name_disp = get_model_type_and_name(active_model)
            logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: ãƒ¢ãƒ‡ãƒ« '{active_model}' ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¤‰æ›´ -> '{prompt_name}' by {interaction.user}")
            await interaction.followup.send(f"âœ… ãƒ¢ãƒ‡ãƒ« **{model_name_disp} ({model_type_disp})** ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š -> **{prompt_name}**ã€‚", ephemeral=True)
            
            public_log_message = f"ğŸ”§ {interaction.user.mention} ãŒãƒ¢ãƒ‡ãƒ« **{model_name_disp} ({model_type_disp})** ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¤‰æ›´: **{current_prompt_name_display}** â†’ **{prompt_name}**"
            try:
                if interaction.channel: await interaction.channel.send(public_log_message)
            except discord.HTTPException as e: logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id}: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¤‰æ›´å…¬é–‹ãƒ­ã‚°é€ä¿¡å¤±æ•—: {e}")
        else:
            model_type_disp, model_name_disp = get_model_type_and_name(active_model)
            await interaction.followup.send(f"â„¹ï¸ ãƒ¢ãƒ‡ãƒ« **{model_name_disp} ({model_type_disp})** ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯æ—¢ã« **{prompt_name}**ã€‚", ephemeral=True)


@bot.tree.command(name="clear_history", description="ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®ä¼šè©±å±¥æ­´ã¨å¿œç­”çµ±è¨ˆã‚’æ¶ˆå»ã—ã¾ã™ã€‚")
async def clear_history_command(interaction: discord.Interaction): 
    target_channel_id = interaction.channel_id
    if target_channel_id != CHAT_CHANNEL_ID:
        await interaction.response.send_message("ã“ã®ã‚³ãƒãƒ³ãƒ‰ã¯æŒ‡å®šã•ã‚ŒãŸãƒãƒ£ãƒƒãƒˆãƒãƒ£ãƒ³ãƒãƒ«ã§ã®ã¿ä½¿ç”¨ã§ãã¾ã™ã€‚", ephemeral=True); return
    
    await interaction.response.defer(ephemeral=True, thinking=False)
    
    if target_channel_id in channel_data:
        channel_data[target_channel_id]["history"].clear()
        channel_data[target_channel_id]["stats"].clear()
        logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ«ID {target_channel_id} ä¼šè©±å±¥æ­´/çµ±è¨ˆã‚¯ãƒªã‚¢å®Œäº† by {interaction.user}ã€‚")
        await interaction.followup.send("âœ… ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®ä¼šè©±å±¥æ­´ã¨å¿œç­”çµ±è¨ˆã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚", ephemeral=True)
    else: 
        await interaction.followup.send("â„¹ï¸ ã‚¯ãƒªã‚¢å¯¾è±¡ã®ä¼šè©±å±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", ephemeral=True)


@bot.tree.command(name="show_history", description="ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®ç›´è¿‘ã®ä¼šè©±å±¥æ­´ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
@app_commands.describe(count=f"è¡¨ç¤ºã™ã‚‹å±¥æ­´ã®ä»¶æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10, æœ€å¤§ {HISTORY_LIMIT})")
async def show_history_command(interaction: discord.Interaction, count: app_commands.Range[int, 1, None] = 10): 
    target_channel_id = interaction.channel_id
    if target_channel_id != CHAT_CHANNEL_ID:
        await interaction.response.send_message("ã“ã®ã‚³ãƒãƒ³ãƒ‰ã¯æŒ‡å®šã•ã‚ŒãŸãƒãƒ£ãƒƒãƒˆãƒãƒ£ãƒ³ãƒãƒ«ã§ã®ã¿ä½¿ç”¨ã§ãã¾ã™ã€‚", ephemeral=True); return

    await interaction.response.defer(ephemeral=True, thinking=False)
    history = channel_data[target_channel_id]["history"]
    if not history:
        await interaction.followup.send("è¡¨ç¤ºã§ãã‚‹ä¼šè©±å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", ephemeral=True); return

    actual_count = min(count, HISTORY_LIMIT, len(history))
    history_list = list(history)
    start_index = max(0, len(history_list) - actual_count)
    display_history = history_list[start_index:]

    embed = Embed(title=f"ç›´è¿‘ä¼šè©±å±¥æ­´ ({len(display_history)}/{len(history_list)}ä»¶)", color=discord.Color.light_gray())
    history_text = ""
    for i, msg in enumerate(display_history):
        prefix = "ğŸ¤–" if msg["is_bot"] else "ğŸ‘¤"
        author_name_safe = discord.utils.escape_markdown(msg['author_name'])
        author_str = f"{prefix} **{'Assistant' if msg['is_bot'] else author_name_safe}**"
        content_short = (msg['content'][:150] + '...') if len(msg['content']) > 150 else msg['content']
        content_safe = discord.utils.escape_markdown(content_short).replace('`', '\\`') 
        entry_text = f"`{start_index + i + 1}`. {author_str}:\n```\n{content_safe}\n```\n" 

        if len(history_text) + len(entry_text) > 3900: 
            history_text += "... (è¡¨ç¤ºæ•°ä¸Šé™ã®ãŸã‚çœç•¥)"
            break
        history_text += entry_text
    
    embed.description = history_text if history_text else "å±¥æ­´å†…å®¹ãŒç©ºã§ã™ã€‚"
    embed.set_footer(text=f"æœ€å¤§ä¿æŒæ•°: {HISTORY_LIMIT}ä»¶")
    await interaction.followup.send(embed=embed, ephemeral=True)

@bot.tree.command(name="list_models", description="åˆ©ç”¨å¯èƒ½ãªå…¨ã¦ã®AIãƒ¢ãƒ‡ãƒ«ã‚’ä¸€è¦§è¡¨ç¤ºã—ã¾ã™ã€‚")
async def list_models_command(interaction: discord.Interaction):
    await interaction.response.defer(ephemeral=True) 

    if not available_bot_models:
        await interaction.followup.send("ç¾åœ¨åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚", ephemeral=True)
        return

    embed = Embed(title="åˆ©ç”¨å¯èƒ½ãªAIãƒ¢ãƒ‡ãƒ«ä¸€è¦§", color=discord.Color.blue())
    
    # ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆã¨åŒã˜ã‚«ãƒ†ã‚´ãƒªåˆ†ã‘ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
    ollama_models_list = []
    gemini_2_5_models_list = []
    gemini_2_0_models_list = []
    gemini_1_5_models_list = []
    gemini_1_0_models_list = []
    gemini_gemma_models_list = []
    other_gemini_models_list = []
    unknown_prefix_models_list = []

    for model_id in available_bot_models: # available_bot_models ã¯ã‚½ãƒ¼ãƒˆæ¸ˆã¿
        if model_id.startswith("ollama:"):
            ollama_models_list.append(model_id)
        elif model_id.startswith("gemini:"):
            name_after_prefix = model_id[len("gemini:"):]
            if name_after_prefix.startswith("gemini-2.5"):
                gemini_2_5_models_list.append(model_id)
            elif name_after_prefix.startswith("gemini-2.0"):
                gemini_2_0_models_list.append(model_id)
            elif name_after_prefix.startswith("gemini-1.5"):
                gemini_1_5_models_list.append(model_id)
            elif name_after_prefix.startswith("gemini-1.0"):
                gemini_1_0_models_list.append(model_id)
            elif name_after_prefix.startswith("gemma"):
                gemini_gemma_models_list.append(model_id)
            else:
                other_gemini_models_list.append(model_id)
        else:
            unknown_prefix_models_list.append(model_id)

    # Embedãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’è¿½åŠ ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (å¤‰æ›´ãªã—ã§å†åˆ©ç”¨å¯èƒ½)
    async def add_models_to_field(title: str, models_in_category: list[str], embed_obj: Embed):
        if not models_in_category:
            if len(embed_obj.fields) < 25:
                embed_obj.add_field(name=title, value="- (ãªã—)", inline=False)
            return

        current_field_text = ""
        field_part_count = 1
        base_title = title

        for i, model_name in enumerate(sorted(models_in_category)): # ã‚«ãƒ†ã‚´ãƒªå†…ã§ã‚½ãƒ¼ãƒˆã—ã¦è¡¨ç¤º
            line_to_add = f"- `{model_name}`\n"
            
            if len(current_field_text) + len(line_to_add) > 1020: 
                if len(embed_obj.fields) < 25:
                    field_title_to_use = f"{base_title} ({field_part_count})" if field_part_count > 1 or (len(models_in_category) - i > 0) else base_title
                    embed_obj.add_field(name=field_title_to_use, value=current_field_text.strip() if current_field_text else " ", inline=False)
                    current_field_text = line_to_add 
                    field_part_count += 1
                else: 
                    if not embed_obj.footer: 
                        embed_obj.set_footer(text="ãƒ¢ãƒ‡ãƒ«å¤šæ•°ã®ãŸã‚ã€ãƒªã‚¹ãƒˆã®ä¸€éƒ¨ãŒçœç•¥ã•ã‚Œã¦ã„ã¾ã™ã€‚")
                    return 
            else:
                current_field_text += line_to_add
        
        if current_field_text and len(embed_obj.fields) < 25:
            field_title_to_use = f"{base_title} ({field_part_count})" if field_part_count > 1 else base_title
            embed_obj.add_field(name=field_title_to_use, value=current_field_text.strip(), inline=False)
        elif current_field_text and not embed_obj.footer: 
             embed_obj.set_footer(text="ãƒ¢ãƒ‡ãƒ«å¤šæ•°ã®ãŸã‚ã€ãƒªã‚¹ãƒˆã®ä¸€éƒ¨ãŒçœç•¥ã•ã‚Œã¦ã„ã¾ã™ã€‚")

    # æŒ‡å®šã•ã‚ŒãŸå„ªå…ˆé †ä½ã§ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
    await add_models_to_field("Ollama Models", ollama_models_list, embed)
    if len(embed.fields) < 25: await add_models_to_field("Gemini 2.5 Series Models", gemini_2_5_models_list, embed)
    if len(embed.fields) < 25: await add_models_to_field("Gemini 2.0 Series Models", gemini_2_0_models_list, embed)
    if len(embed.fields) < 25: await add_models_to_field("Gemini 1.5 Series Models", gemini_1_5_models_list, embed)
    if len(embed.fields) < 25: await add_models_to_field("Gemini 1.0 Series Models", gemini_1_0_models_list, embed)
    if len(embed.fields) < 25: await add_models_to_field("Gemini Gemma Models", gemini_gemma_models_list, embed)
    if len(embed.fields) < 25: await add_models_to_field("Other Gemini Models", other_gemini_models_list, embed)
    if unknown_prefix_models_list and len(embed.fields) < 25:
        await add_models_to_field("Unknown Prefix Models", unknown_prefix_models_list, embed)
    
    if not embed.fields and not available_bot_models: # æœ€åˆã® available_bot_models ãƒã‚§ãƒƒã‚¯ã§æ•æ‰ã•ã‚Œã‚‹ãŒå¿µã®ãŸã‚
        embed.description = "è¡¨ç¤ºå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
    elif not embed.fields and available_bot_models : # ã‚«ãƒ†ã‚´ãƒªåˆ†ã‘ã•ã‚ŒãŸãŒã€ä½•ã‚‰ã‹ã®ç†ç”±ã§ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒä¸€ã¤ã‚‚ä½œã‚‰ã‚Œãªã‹ã£ãŸå ´åˆ
        embed.description = "ãƒ¢ãƒ‡ãƒ«ã¯ã‚ã‚Šã¾ã™ãŒã€ã‚«ãƒ†ã‚´ãƒªè¡¨ç¤ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚"


    try:
        await interaction.followup.send(embed=embed, ephemeral=True)
    except discord.HTTPException as e:
        logger.error(f"/list_models ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œä¸­ã«Discord APIã‚¨ãƒ©ãƒ¼: {e}")
        await interaction.followup.send("ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã®è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒªã‚¹ãƒˆãŒå¤§ãã™ãã‚‹ã‹ã€äºˆæœŸã›ã¬å•é¡ŒãŒç™ºç”Ÿã—ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", ephemeral=True)


@bot.tree.command(name="set_param", description="LLMã®ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿(temperature, top_k, top_p)ã‚’èª¿æ•´ã—ã¾ã™ã€‚")
@app_commands.describe(parameter="èª¿æ•´ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å", value="è¨­å®šã™ã‚‹å€¤ (ä¾‹: 0.7, 50)ã€‚æœªè¨­å®šã«æˆ»ã™å ´åˆã¯ 'none' ã¾ãŸã¯ 'default' ã¨å…¥åŠ›ã€‚")
@app_commands.choices(parameter=[
    app_commands.Choice(name="temperature", value="temperature"),
    app_commands.Choice(name="top_k", value="top_k"),
    app_commands.Choice(name="top_p", value="top_p"),
])
async def set_parameter(interaction: discord.Interaction, parameter: app_commands.Choice[str], value: str):
    target_channel_id = interaction.channel_id
    if target_channel_id != CHAT_CHANNEL_ID:
        await interaction.response.send_message("ã“ã®ã‚³ãƒãƒ³ãƒ‰ã¯æŒ‡å®šã•ã‚ŒãŸãƒãƒ£ãƒƒãƒˆãƒãƒ£ãƒ³ãƒãƒ«ã§ã®ã¿ä½¿ç”¨ã§ãã¾ã™ã€‚", ephemeral=True); return
    
    await interaction.response.defer(ephemeral=True, thinking=False)
    
    param_name = parameter.value
    current_params = channel_data[target_channel_id]["params"]
    response_message = ""
    original_value = current_params.get(param_name) 

    try:
        new_value_internal: float | int | None = None 
        
        if value.lower() in ['none', 'default', 'null', '']: 
            new_value_internal = None
        elif param_name == "temperature":
            try:
                float_val = float(value)
                if 0.0 <= float_val <= 2.0: new_value_internal = float_val
                else: raise ValueError("Temperature ã¯ 0.0 ã‹ã‚‰ 2.0 ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
            except ValueError: raise ValueError("Temperature ã«ã¯æ•°å€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        elif param_name == "top_k":
            try:
                int_val = int(value)
                if int_val >= 0: new_value_internal = int_val 
                else: raise ValueError("Top K ã¯ 0 ä»¥ä¸Šã®æ•´æ•°ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
            except ValueError: raise ValueError("Top K ã«ã¯æ•´æ•°ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        elif param_name == "top_p":
            try:
                float_val = float(value)
                if 0.0 <= float_val <= 1.0: new_value_internal = float_val
                else: raise ValueError("Top P ã¯ 0.0 ã‹ã‚‰ 1.0 ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
            except ValueError: raise ValueError("Top P ã«ã¯æ•°å€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

        is_changed = False
        if original_value is None and new_value_internal is not None: is_changed = True
        elif original_value is not None and new_value_internal is None: is_changed = True
        elif isinstance(original_value, (int, float)) and isinstance(new_value_internal, (int, float)):
            if not math.isclose(original_value, new_value_internal, rel_tol=1e-9): is_changed = True
        elif original_value != new_value_internal : # ä¸Šè¨˜ä»¥å¤– (é€šå¸¸ã¯ type ãŒç•°ãªã‚‹å ´åˆãªã©)
            is_changed = True


        if is_changed:
            current_params[param_name] = new_value_internal
            logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ« {target_channel_id}: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ '{param_name}' è¨­å®š -> '{new_value_internal}' by {interaction.user}")
            response_message = f"âœ… ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ **{param_name}** è¨­å®š -> **{new_value_internal if new_value_internal is not None else 'æœªè¨­å®š (APIãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)'}**ã€‚"
        else:
            response_message = f"â„¹ï¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ **{param_name}** ã¯æ—¢ã« **{original_value if original_value is not None else 'æœªè¨­å®š (APIãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)'}**ã€‚"

    except ValueError as e:
        logger.warning(f"ãƒãƒ£ãƒ³ãƒãƒ« {target_channel_id}: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã‚¨ãƒ©ãƒ¼ ({param_name}={value}): {e} by {interaction.user}")
        response_message = f"âš ï¸ è¨­å®šå€¤ã‚¨ãƒ©ãƒ¼: {e}"
    except Exception as e:
        logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ« {target_channel_id}: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šä¸­ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        response_message = "âŒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
        
    await interaction.followup.send(response_message, ephemeral=True)


@bot.tree.command(name="stats", description="ç¾åœ¨ã®è¨­å®šã¨ç›´è¿‘ã®å¿œç­”ç”Ÿæˆçµ±è¨ˆã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
async def show_stats_command(interaction: discord.Interaction): 
    target_channel_id = interaction.channel_id
    if target_channel_id != CHAT_CHANNEL_ID:
        await interaction.response.send_message("ã“ã®ã‚³ãƒãƒ³ãƒ‰ã¯æŒ‡å®šã•ã‚ŒãŸãƒãƒ£ãƒƒãƒˆãƒãƒ£ãƒ³ãƒãƒ«ã§ã®ã¿ä½¿ç”¨ã§ãã¾ã™ã€‚", ephemeral=True); return

    await interaction.response.defer(ephemeral=True, thinking=False)

    stats_deque = channel_data[target_channel_id]["stats"]
    total_stats_count = len(stats_deque)
    stats_max_len = channel_data[target_channel_id]["stats"].maxlen or 50

    embed = Embed(title="ğŸ“Š BOTã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ & å¿œç­”çµ±è¨ˆ", color=discord.Color.green())

    active_model_type, active_model_name = get_model_type_and_name(active_model)
    current_model_str = f"**{active_model_name or 'N/A'} ({active_model_type or 'N/A'})**" if active_model else "æœªè¨­å®š"
    
    current_prompt_name_str = "N/A"
    if active_model:
        current_prompt_content = system_prompts.get(active_model)
        current_prompt_name_str = f"**{get_prompt_name_from_content(current_prompt_content)}**"
    
    current_params = channel_data[target_channel_id]["params"]
    params_str_parts = []
    if current_params.get("temperature") is not None: params_str_parts.append(f"Temp={current_params['temperature']}")
    if current_params.get("top_k") is not None: params_str_parts.append(f"TopK={current_params['top_k']}")
    if current_params.get("top_p") is not None: params_str_parts.append(f"TopP={current_params['top_p']}")
    params_str = ", ".join(params_str_parts) if params_str_parts else "APIãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ"

    embed.add_field(
        name="ç¾åœ¨ã®è¨­å®š",
        value=f"ãƒ¢ãƒ‡ãƒ«: {current_model_str}\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {current_prompt_name_str}\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: `{params_str}`",
        inline=False
    )

    if not stats_deque:
        embed.add_field(name=f"å¿œç­”çµ±è¨ˆ (ç›´è¿‘ 0/{stats_max_len} å›)", value="è¨˜éŒ²ãªã—ã€‚", inline=False)
    else:
        total_duration_sum, total_generated_tokens_sum, total_tps_sum, valid_tps_entries = 0.0, 0, 0.0, 0
        
        for stat_entry in stats_deque:
            duration = stat_entry.get("total_duration", 0.0)
            if 0.01 < duration < 600 : total_duration_sum += duration

            generated_tokens = stat_entry.get("total_tokens", stat_entry.get("generated_tokens", 0)) # Ollama or Gemini
            if generated_tokens > 0: total_generated_tokens_sum += generated_tokens
            
            tps = stat_entry.get("tokens_per_second", 0.0)
            if 0.01 < tps < 10000 : 
                total_tps_sum += tps
                valid_tps_entries += 1
        
        avg_duration = total_duration_sum / total_stats_count if total_stats_count > 0 else 0.0
        avg_generated_tokens = total_generated_tokens_sum / total_stats_count if total_stats_count > 0 else 0.0
        avg_tps = total_tps_sum / valid_tps_entries if valid_tps_entries > 0 else 0.0

        stats_summary = (
            f"å¹³å‡å¿œç­”æ™‚é–“: **{avg_duration:.2f} ç§’**\n"
            f"å¹³å‡ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: **{avg_generated_tokens:.1f} ãƒˆãƒ¼ã‚¯ãƒ³**\n"
            f"å¹³å‡TPS (æœ‰åŠ¹ãªã‚¨ãƒ³ãƒˆãƒªã®ã¿): **{avg_tps:.2f} tok/s**"
        )
        embed.add_field(name=f"å¿œç­”çµ±è¨ˆ (ç›´è¿‘ {total_stats_count}/{stats_max_len} å›)", value=stats_summary, inline=False)

    api_urls = [f"Ollama: {OLLAMA_API_URL}"]
    if genai: api_urls.append("Gemini: (Google Cloud)") 
    
    embed.set_footer(text=f"å±¥æ­´ä¿æŒæ•°: {HISTORY_LIMIT} | APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: {', '.join(api_urls)}")
    await interaction.followup.send(embed=embed, ephemeral=True)


# --- BOTèµ·å‹• ---
if __name__ == "__main__":
    if not TOKEN: logger.critical("ç’°å¢ƒå¤‰æ•° 'DISCORD_TOKEN' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚BOTã‚’çµ‚äº†ã—ã¾ã™ã€‚"); sys.exit(1)
    if CHAT_CHANNEL_ID is None: logger.critical("ç’°å¢ƒå¤‰æ•° 'CHAT_CHANNEL_ID' ãŒç„¡åŠ¹ã§ã™ã€‚BOTã‚’çµ‚äº†ã—ã¾ã™ã€‚"); sys.exit(1) 
    if aiofiles is None: logger.warning("`aiofiles` ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä¸€éƒ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«é–¢é€£æ©Ÿèƒ½ãŒåˆ¶é™ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    
    if genai is not None and not GEMINI_API_KEY:
        logger.warning("Geminiãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã‚ã‚Šã¾ã™ãŒã€ç’°å¢ƒå¤‰æ•° 'GEMINI_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Gemini APIæ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
    elif genai is None and GEMINI_API_KEY: 
         logger.warning("ç’°å¢ƒå¤‰æ•° 'GEMINI_API_KEY' ã¯è¨­å®šã•ã‚Œã¦ã„ã¾ã™ãŒã€'google-generativeai'ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚Gemini APIæ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")


    logger.info("--- LLM Discord BOT èµ·å‹•ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹ ---")
    logger.info(f"ç›£è¦–ãƒãƒ£ãƒ³ãƒãƒ«ID: {CHAT_CHANNEL_ID}")
    logger.info(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«: {DEFAULT_MODEL or 'æœªè¨­å®š (èµ·å‹•æ™‚ã«è‡ªå‹•é¸æŠè©¦è¡Œ)'}")
    logger.info(f"å±¥æ­´ä¿æŒæ•°: {HISTORY_LIMIT}")
    logger.info(f"Ollama API URL: {OLLAMA_API_URL}")
    if GEMINI_API_KEY and genai:
        logger.info("Gemini APIã‚­ãƒ¼: è¨­å®šæ¸ˆã¿ (Gemini API åˆ©ç”¨å¯èƒ½)")
    elif GEMINI_API_KEY and not genai:
        logger.info("Gemini APIã‚­ãƒ¼: è¨­å®šæ¸ˆã¿ (ãŸã ã—ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—ã®ãŸã‚Gemini API åˆ©ç”¨ä¸å¯)")
    else:
        logger.info("Gemini APIã‚­ãƒ¼: æœªè¨­å®š (Gemini API åˆ©ç”¨ä¸å¯)")

    logger.info(f"ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆDir: {prompts_dir_path}")
    logger.info(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒªãƒ­ãƒ¼ãƒ‰é–“éš”: {PROMPT_RELOAD_INTERVAL_MINUTES} åˆ†")
    logger.info(f"ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆæ›´æ–°é–“éš”: {MODEL_UPDATE_INTERVAL_MINUTES} åˆ†")
    logger.info("-------------------------------------------")

    try:
        bot.run(TOKEN, log_handler=None) 
    except discord.LoginFailure:
        logger.critical("Discordã¸ã®ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸã€‚DISCORD_TOKENãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except discord.PrivilegedIntentsRequired:
        logger.critical("å¿…è¦ãªPrivileged Intents (ç‰¹ã«Message Content Intent) ãŒDiscord Developer Portalã§æœ‰åŠ¹ã«ãªã£ã¦ã„ã¾ã›ã‚“ã€‚ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except ImportError as e: 
        logger.critical(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    except Exception as e:
        logger.critical(f"BOTèµ·å‹•ä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)

# --- END OF FILE bot.py ---